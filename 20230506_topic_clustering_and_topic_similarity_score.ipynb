{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "from analytics.read import *\n",
    "from analytics.preprocess import *\n",
    "from analytics.analysis import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input benchmark and target dataset list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_target = dict()\n",
    "\n",
    "dict_target['benchmark'] = ['일취월장', '완공', '뼈아대'] \n",
    "dict_target['일취월장'] = ['일취월장'] \n",
    "dict_target['완공'] = ['완공'] \n",
    "dict_target['뼈아대'] = ['뼈아대'] \n",
    "\n",
    "dict_target['personal'] = ['Daily_Report', 'Weekly_Report', 'Monthly_Report', '글쓰기'] \n",
    "dict_target['멘토_브런치'] = ['멘토_브런치']\n",
    "dict_target['신박사_브런치'] = ['신박사_브런치']\n",
    "dict_target['신영준_페이스북'] = ['신영준_페이스북']\n",
    "dict_target['연예_기사'] = ['연예_기사'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "files = glob('text/*/*.txt')\n",
    "names = {file.split('/')[1] for file in files}\n",
    "\n",
    "dict_target = dict()\n",
    "\n",
    "for name in names:\n",
    "    dict_target[name] = [str(i) for i in range(1, 8)] \n",
    "\n",
    "# dict_target['경규승'] = [str(i) for i in range(1, 8)] \n",
    "# dict_target['경규승'] = [str(i) for i in range(1, 8)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .txt 읽은 dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_target.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_strings = get_dict_strings(dict_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_merged_strings = get_dict_merged_strings(dict_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_merged_strings['이정애']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .txt을 읽은 dict -> noun의 list로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_nouns = get_nouns_from_topics(dict_merged_strings, komoran)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dict_nouns.keys():\n",
    "    print('# of nouns in {0}: {1}'.format(key, len(dict_nouns[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for category in dict_nouns.keys():\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    plt.title(category)\n",
    "\n",
    "    # 30개 noun 까지만 보여줌.\n",
    "    Text(dict_nouns[category], name=category).plot(70)\n",
    "    plt.savefig(f'output/{category}_original_graph.png')\n",
    "    # plt.show()\n",
    "    # plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> 지수적으로 떨어지는게 자연적인 모양. 오히려 지수적으로 떨어지지 않으면 데이터가 부족한 것이라 판단."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## noun 빈도수를 포함한 dict로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_frequency = get_nouns_frequency(dict_nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "의미 부족한 단어 제거 (10 단어 이상의 경우만 체크함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_words_benchmark = ['것', '수', '때', '때문', '경우', '필요', '가지', '정도', '뿐', '년', \n",
    "                          '등', '데' '개', '대부분', '번', '점', '관련', '전', '중', '자', '분', \n",
    "                          '만', '게', '동안', '뜻', '곳', '언가']\n",
    "delete_words_personal = ['것', '수', '때', '안', '때문', '건', '필요', '곳', '번', '전', '중', '정도',\n",
    "                         '년', '거', '점', '.so', '라고', '부분', '구체', '뿐', '자체', '가지', '밖',\n",
    "                         '날', '분', '.com', '데', '개', '의', '을', '자', '션', '한', '회', '줄', '영',\n",
    "                         '만', '명', '적', '시']\n",
    "delete_words_huge_portion = ['사람', '자신', '일', '말', '생각', '책', '페이지', '속', '경', '듯', '조금']\n",
    "delete_words = delete_words_benchmark + delete_words_personal + delete_words_huge_portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_nouns_ppd = get_preprocessed_nouns(dict_nouns, delete_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for category in dict_nouns_ppd.keys():\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    plt.title(category)\n",
    "    Text(dict_nouns_ppd[category], name=category).plot(70)\n",
    "    plt.savefig(f'output/{category}_ppd_graph.png')\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_frequency_ppd = get_preprocessed_nouns_frequency(dict_frequency, delete_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "mask = np.array(PIL.Image.open(\"vocation.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font_path = '/Users/gs/Library/Fonts/NanumBarunGothic.otf'\n",
    "for category in dict_frequency_ppd.keys():\n",
    "    # wc = WordCloud(width = 1500, height = 1500, background_color=\"white\", font_path=font_path)\n",
    "    wc = WordCloud(\n",
    "                   # width=800,\n",
    "                   # height=300,\n",
    "                   background_color=\"white\",\n",
    "                   mask=mask,\n",
    "                   font_path=font_path,\n",
    "                   max_words=150,\n",
    "                   max_font_size=150,\n",
    "                   random_state=42,\n",
    "                   contour_width=0,\n",
    "                   contour_color='steelblue',\n",
    "                   colormap='tab20')\n",
    "    # plt.figure(figsize=(10,60))\n",
    "    plt.title(category, fontsize=15)\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.imshow(wc.generate_from_frequencies(dict_frequency_ppd[category]))\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(f'output/{category}_ppd_wc.png')\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# font_path = '/Users/gs/Library/Fonts/NanumBarunGothic.otf'\n",
    "# for category in list(dict_frequency_ppd.keys())[:1]:\n",
    "#     # wc = WordCloud(width = 1500, height = 1500, background_color=\"white\", font_path=font_path)\n",
    "#     wc = WordCloud(width=1500,\n",
    "#                    height=1500,\n",
    "#                    background_color=\"white\",\n",
    "#                    font_path=font_path,\n",
    "#                    max_words=150,\n",
    "#                    max_font_size=150,\n",
    "#                    random_state=42,\n",
    "#                    contour_width=3,\n",
    "#                    # contour_color='steelblue',\n",
    "#                    colormap='Blues')\n",
    "#     plt.figure( figsize=(20,10))\n",
    "#     plt.title(category, fontsize=25)\n",
    "#     plt.imshow(wc.generate_from_frequencies(dict_frequency_ppd[category]))\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.savefig(f'output/{key}_ppd_wc.png')\n",
    "#     # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sse_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sse = pd.DataFrame()\n",
    "for category_a in dict_frequency_ppd.keys():\n",
    "    for category_b in dict_frequency_ppd.keys():\n",
    "        df_sse = df_sse.append([[category_a, category_b,\n",
    "                                get_score(\n",
    "                                    dict_frequency_ppd[category_a], dict_frequency_ppd[category_b], 'sse')]],\n",
    "                               ignore_index=True)\n",
    "df_sse.columns = ['category_a', 'category_b', 'score']\n",
    "df_sse.pivot(index='category_a', columns='category_b', values='score')\n",
    "# pd.pivot_table(df_sse, index='category_a', columns='category_b', values='score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sse.pivot(index='category_a', columns='category_b', values='score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_sse -> log는 의도와는 좀 다르게 나오는 것 같음. 전처리 의도는 맞는데 왜 그럴까?\n",
    "df_sse = pd.DataFrame()\n",
    "for category_a in dict_frequency_ppd.keys():\n",
    "    for category_b in dict_frequency_ppd.keys():\n",
    "        df_sse = df_sse.append([[category_a, category_b,\n",
    "                                get_score(\n",
    "                                    dict_frequency_ppd[category_a], dict_frequency_ppd[category_b], 'log_sse')]],\n",
    "                               ignore_index=True)\n",
    "df_sse.columns = ['category_a', 'category_b', 'score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sse.pivot(index='category_a', columns='category_b', values='score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for column order\n",
    "df_sse_pivot = df_sse.pivot(index='category_a', columns='category_b', values='score')\n",
    "list_order = ['benchmark',\n",
    "              'personal',\n",
    "              '신박사_브런치',\n",
    "              '신영준_페이스북',\n",
    "              '멘토_브런치',\n",
    "              '연예_기사',\n",
    "              '뼈아대',\n",
    "              '일취월장',\n",
    "              '완공']\n",
    "df_sse_pivot[list_order].reindex(index=list_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decide # of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "category = 'benchmark'\n",
    "for sentence in [s for s in dict_merged_strings[category].split(\"\\n\") if s]:\n",
    "    documents.append(' '.join([s for s in komoran.nouns(sentence) if s]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = [i for i in range(2, 25)]\n",
    "learning_decays = [round(0.35 + 0.05 * i, 2) for i in range(0, 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warning 은 .com , .so 제거하는 과정에서 발생.\n",
    "\n",
    "list_perplexity = []\n",
    "\n",
    "for ld in learning_decays:\n",
    "    for n_components in n_topics:\n",
    "        tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words=delete_words)\n",
    "        tf = tf_vectorizer.fit_transform(documents)\n",
    "        tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "        # Run LDA\n",
    "        lda = LatentDirichletAllocation(learning_decay = ld,\n",
    "                                        n_components=n_components, max_iter=5, \n",
    "                                        learning_method='online', learning_offset=50.,random_state=0,\n",
    "                                        doc_topic_prior=0.1, topic_word_prior=0.01,\n",
    "                                       ).fit(tf)\n",
    "        # perplexity\n",
    "        tf_perplexity = lda.perplexity(tf)\n",
    "        \n",
    "        list_perplexity.append([n_components, ld, tf_perplexity])\n",
    "\n",
    "df_perplexity = pd.DataFrame(list_perplexity, columns=['n_topics', 'learning_decay', 'perplexity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i in range(len(learning_decays)):\n",
    "    plt.plot(n_topics, df_perplexity[df_perplexity.learning_decay==learning_decays[i]].perplexity, \n",
    "             label=learning_decays[i])\n",
    "\n",
    "plt.title(\"Choosing Optimal LDA Model\")\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"perplexity\")\n",
    "plt.legend(title='Learning decay', loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cluster 숫자 결정  \n",
    "decided_n_components = 21  \n",
    "decided_ld = 0.45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### topic clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클러스터 수, learning_decay 결정\n",
    "decided_n_components = 21\n",
    "decided_ld = 0.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda, lda_tf_feature_names = get_lda_model(dict_merged_strings, decided_ld, decided_n_components, \n",
    "                                          delete_words, category='benchmark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# display\n",
    "no_top_words = 10\n",
    "display_topics(lda, lda_tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_topic_clustering = get_lda_topic_clustering(lda, lda_tf_feature_names)\n",
    "df_topic_clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 토픽별 단어 숫자\n",
    "df_topic_clustering.topic_words.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label 붙이기.\n",
    "df_topic_clustering_label = df_topic_clustering.copy()\n",
    "df_topic_clustering_label['topic_label'] = [\n",
    "    '계획', \n",
    "    '학습', \n",
    "    '능력', \n",
    "    '마음',\n",
    "    '세상',\n",
    "    '정보와 아이디어',\n",
    "    '믿음',\n",
    "    '습관',\n",
    "    '성공과 도전',\n",
    "    '공부',\n",
    "    '행복',\n",
    "    '회사내 태도',\n",
    "    '실력',\n",
    "    '기억',\n",
    "    '목표',\n",
    "    '결과',\n",
    "    '창의',\n",
    "    '행동',\n",
    "    '호기심',\n",
    "    '조직',\n",
    "    '메타인지',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_clustering_label[['topic_label', 'topic_words']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## topic sse score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_scores = pd.DataFrame()\n",
    "for category in ['personal', '뼈아대', '일취월장', '완공', '신박사_브런치', '신영준_페이스북', '멘토_브런치', '연예_기사']:\n",
    "    df_topic_score = get_topic_scores(\n",
    "        dict_frequency_ppd['benchmark'], dict_frequency_ppd[category], df_topic_clustering)\n",
    "    df_topic_score.columns = ['{}_score'.format(category)]\n",
    "    \n",
    "    df_topic_scores = pd.concat([df_topic_scores, df_topic_score], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_label_score = pd.merge(df_topic_clustering_label[['topic_label']], df_topic_scores,\n",
    "                                left_index=True, right_index=True)\n",
    "df_topic_label_score.reset_index(inplace=True)\n",
    "df_topic_label_score.set_index(['topic_index', 'topic_label'], inplace=True)\n",
    "\n",
    "df_topic_label_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_bottom_topic = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_n_topics = pd.DataFrame()\n",
    "for column in df_topic_label_score.columns:\n",
    "    df_top_n_topics = df_top_n_topics.append(\n",
    "        [['_'.join(column.split('_')[:-1]),\n",
    "         df_topic_label_score.reset_index().nlargest(\n",
    "             n_top_bottom_topic, column).topic_label.to_list(),\n",
    "         df_topic_label_score.reset_index().nlargest(\n",
    "             n_top_bottom_topic, column)[column].mean()]], ignore_index=True)\n",
    "df_top_n_topics.columns = ['category', 'strong_topics', 'strong_topics_mean_scores']\n",
    "df_top_n_topics.set_index('category', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bottom_n_topics = pd.DataFrame()\n",
    "for column in df_topic_label_score.columns:\n",
    "    df_bottom_n_topics = df_bottom_n_topics.append(\n",
    "        [['_'.join(column.split('_')[:-1]),\n",
    "         df_topic_label_score.reset_index().nsmallest(\n",
    "             n_top_bottom_topic, column).topic_label.to_list(),\n",
    "         df_topic_label_score.reset_index().nsmallest(\n",
    "             n_top_bottom_topic, column)[column].mean()]], ignore_index=True)\n",
    "df_bottom_n_topics.columns = ['category', 'weak_topics', 'weak_topics_mean_scores']\n",
    "df_bottom_n_topics.set_index('category', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_strong_weak_topics = pd.merge(df_top_n_topics, df_bottom_n_topics, \n",
    "                                 left_index=True, right_index=True)\n",
    "df_strong_weak_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
